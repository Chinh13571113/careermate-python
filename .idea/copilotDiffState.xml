<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/CLOUD_RUN_ENV_SETUP.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CLOUD_RUN_ENV_SETUP.md" />
              <option name="updatedContent" value="# Cloud Run Environment Variables Setup&#10;&#10;## Vấn đề: Connection Error khi gọi OpenAI API&#10;&#10;Cloud Run **KHÔNG** chặn kết nối đến OpenAI API, nhưng có thể gặp lỗi do:&#10;&#10;1. **Thiếu biến môi trường OPENAI_API_KEY**&#10;2. **Base URL tùy chỉnh không khả dụng từ Cloud Run**&#10;3. **Timeout quá ngắn**&#10;4. **Network latency cao hơn local**&#10;&#10;## Giải pháp&#10;&#10;### 1. Thiết lập biến môi trường trên Cloud Run&#10;&#10;Chạy lệnh sau để cập nhật biến môi trường:&#10;&#10;```bash&#10;gcloud run services update careermate-python \&#10;  --region=asia-southeast1 \&#10;  --set-env-vars=&quot;OPENAI_API_KEY=your-openai-api-key-here,OPENAI_BASE_URL=https://aiportalapi.stu-platform.live/jpe,OPENAI_MODEL=gpt-4o&quot;&#10;```&#10;&#10;### 2. Kiểm tra biến môi trường hiện tại&#10;&#10;```bash&#10;gcloud run services describe careermate-python --region=asia-southeast1 --format=&quot;value(spec.template.spec.containers[0].env)&quot;&#10;```&#10;&#10;### 3. Tăng timeout cho Cloud Run&#10;&#10;Cloud Run mặc định có timeout 60 giây cho startup. Nếu cần tăng:&#10;&#10;```bash&#10;gcloud run services update careermate-python \&#10;  --region=asia-southeast1 \&#10;  --timeout=300 \&#10;  --cpu=2 \&#10;  --memory=2Gi&#10;```&#10;&#10;### 4. Kiểm tra logs để debug&#10;&#10;```bash&#10;gcloud run services logs read careermate-python \&#10;  --region=asia-southeast1 \&#10;  --limit=100&#10;```&#10;&#10;## Các biến môi trường cần thiết&#10;&#10;| Variable | Required | Description |&#10;|----------|----------|-------------|&#10;| `OPENAI_API_KEY` | ✅ Yes | API key từ OpenAI hoặc proxy |&#10;| `OPENAI_BASE_URL` | ⚠️ Optional | Mặc định: `https://aiportalapi.stu-platform.live/jpe` |&#10;| `OPENAI_MODEL` | ⚠️ Optional | Mặc định: `gpt-4o` |&#10;| `GOOGLE_API_KEY` | ✅ Yes | API key cho Gemini |&#10;| `WEAVIATE_URL` | ✅ Yes | Weaviate Cloud URL |&#10;| `WEAVIATE_API_KEY` | ✅ Yes | Weaviate API key |&#10;&#10;## Test connection từ Cloud Run&#10;&#10;Sau khi deploy, test API endpoint:&#10;&#10;```bash&#10;curl -X POST https://careermate-python-xxx.run.app/api/v1/cv-analysis/analyze \&#10;  -H &quot;Content-Type: multipart/form-data&quot; \&#10;  -F &quot;cv_file=@test.pdf&quot; \&#10;  -F &quot;job_description=Software Engineer position&quot;&#10;```&#10;&#10;## Troubleshooting&#10;&#10;### Lỗi: &quot;Connection error&quot;&#10;&#10;**Nguyên nhân:**&#10;- Base URL không thể kết nối từ Cloud Run&#10;- API key không hợp lệ&#10;- Firewall/Network issue&#10;&#10;**Giải pháp:**&#10;1. Kiểm tra Base URL có accessible từ internet không&#10;2. Test với OpenAI official URL thay vì proxy:&#10;   ```bash&#10;   gcloud run services update careermate-python \&#10;     --set-env-vars=&quot;OPENAI_BASE_URL=https://api.openai.com/v1&quot;&#10;   ```&#10;&#10;### Lỗi: &quot;OPENAI_API_KEY not found&quot;&#10;&#10;**Giải pháp:**&#10;```bash&#10;gcloud run services update careermate-python \&#10;  --set-env-vars=&quot;OPENAI_API_KEY=sk-your-key-here&quot;&#10;```&#10;&#10;### Timeout quá ngắn&#10;&#10;**Giải pháp:** Code đã được cập nhật với:&#10;- `timeout=60` seconds&#10;- `max_retries=3`&#10;- `request_timeout=60`&#10;&#10;## Cấu hình Secret Manager (Recommended)&#10;&#10;Thay vì dùng env vars trực tiếp, nên dùng Secret Manager:&#10;&#10;```bash&#10;# Tạo secret&#10;echo -n &quot;your-openai-key&quot; | gcloud secrets create openai-api-key --data-file=-&#10;&#10;# Mount secret vào Cloud Run&#10;gcloud run services update careermate-python \&#10;  --update-secrets=OPENAI_API_KEY=openai-api-key:latest&#10;```&#10;&#10;## Monitoring&#10;&#10;Xem logs realtime:&#10;```bash&#10;gcloud run services logs tail careermate-python --region=asia-southeast1&#10;```&#10;&#10;## Changes Made&#10;&#10;### 1. `agent_core/llm.py`&#10;- ✅ Tăng `timeout` từ 30 → 60 seconds&#10;- ✅ Thêm `max_retries=3`&#10;- ✅ Thêm `request_timeout=60`&#10;- ✅ Thêm comprehensive logging&#10;&#10;### 2. `apps/cv_analysis_agent/services/ai_checker_resume_service.py`&#10;- ✅ Thêm error logging chi tiết&#10;- ✅ Thêm fallback JSON khi lỗi&#10;- ✅ Log environment variables khi có lỗi&#10;&#10;## Next Steps&#10;&#10;1. **Thiết lập biến môi trường** trên Cloud Run (bước 1 trên)&#10;2. **Deploy lại** với code mới&#10;3. **Kiểm tra logs** để xem thông tin kết nối&#10;4. Nếu vẫn lỗi, thử **đổi sang OpenAI official URL** thay vì proxy&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/agent_core/llm.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/agent_core/llm.py" />
              <option name="originalContent" value="# agent_core/utils/llm.py&#10;import os&#10;from functools import lru_cache&#10;from dotenv import load_dotenv&#10;from langchain_google_genai import ChatGoogleGenerativeAI&#10;from langchain_openai import ChatOpenAI&#10;&#10;load_dotenv()&#10;&#10;MODEL_NAME = &quot;gemini-2.5-flash&quot;&#10;OPENAI_MODEL_NAME = &quot;gpt-4o&quot;&#10;google_api_key = os.getenv(&quot;GOOGLE_API_KEY&quot;)&#10;openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)&#10;openai_base_url = os.getenv(&quot;OPENAI_BASE_URL&quot;, &quot;https://aiportalapi.stu-platform.live/jpe&quot;)&#10;&#10;if not google_api_key:&#10;    raise ValueError(&quot;❌ GOOGLE_API_KEY not found in environment variables.&quot;)&#10;&#10;&#10;@lru_cache(maxsize=4)&#10;def get_gemini_model(&#10;    temperature: float = 0,&#10;    top_p: float = 1.0,&#10;&#10;&#10;):&#10;    &quot;&quot;&quot;&#10;    Create a cached, configured ChatGoogleGenerativeAI model.&#10;&#10;    Params:&#10;        temperature: randomness (0 = deterministic, 1 = creative)&#10;        top_p: nucleus sampling (controls diversity)&#10;&#10;    Returns:&#10;        Cached LLM instance for better performance&#10;    &quot;&quot;&quot;&#10;    # Configuration for faster responses&#10;    llm = ChatGoogleGenerativeAI(&#10;        model=MODEL_NAME,&#10;        api_key=google_api_key,&#10;        temperature=temperature,&#10;        top_p=top_p,&#10;        max_tokens=1024,&#10;        timeout=15,&#10;&#10;    )&#10;&#10;    return llm&#10;&#10;&#10;@lru_cache(maxsize=4)&#10;def get_openai_model(&#10;    temperature: float = 0,&#10;    top_p: float = 1.0,&#10;):&#10;    &quot;&quot;&quot;&#10;    Create a cached, configured ChatOpenAI model.&#10;&#10;    Params:&#10;        temperature: randomness (0 = deterministic, 1 = creative)&#10;        top_p: nucleus sampling (controls diversity)&#10;&#10;    Returns:&#10;        Cached OpenAI LLM instance for better performance&#10;    &quot;&quot;&quot;&#10;    if not openai_api_key:&#10;        raise ValueError(&quot;❌ OPENAI_API_KEY not found in environment variables.&quot;)&#10;&#10;    llm = ChatOpenAI(&#10;        model=OPENAI_MODEL_NAME,&#10;        api_key=openai_api_key,&#10;        base_url=openai_base_url,&#10;        temperature=temperature,&#10;        top_p=top_p,&#10;        max_tokens=2048,&#10;        timeout=30,&#10;        seed=1234,&#10;    )&#10;&#10;    return llm&#10;" />
              <option name="updatedContent" value="# agent_core/utils/llm.py&#13;&#10;import os&#13;&#10;from functools import lru_cache&#13;&#10;from dotenv import load_dotenv&#13;&#10;from langchain_google_genai import ChatGoogleGenerativeAI&#13;&#10;from langchain_openai import ChatOpenAI&#13;&#10;import logging&#13;&#10;&#13;&#10;load_dotenv()&#13;&#10;&#13;&#10;logger = logging.getLogger(__name__)&#13;&#10;&#13;&#10;MODEL_NAME = &quot;gemini-2.5-flash&quot;&#13;&#10;OPENAI_MODEL_NAME = &quot;gpt-4o&quot;&#13;&#10;google_api_key = os.getenv(&quot;GOOGLE_API_KEY&quot;)&#13;&#10;openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)&#13;&#10;openai_base_url = os.getenv(&quot;OPENAI_BASE_URL&quot;, &quot;https://aiportalapi.stu-platform.live/jpe&quot;)&#13;&#10;&#13;&#10;if not google_api_key:&#13;&#10;    raise ValueError(&quot;❌ GOOGLE_API_KEY not found in environment variables.&quot;)&#13;&#10;&#13;&#10;&#13;&#10;@lru_cache(maxsize=4)&#13;&#10;def get_gemini_model(&#13;&#10;    temperature: float = 0,&#13;&#10;    top_p: float = 1.0,&#13;&#10;&#13;&#10;&#13;&#10;):&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    Create a cached, configured ChatGoogleGenerativeAI model.&#13;&#10;&#13;&#10;    Params:&#13;&#10;        temperature: randomness (0 = deterministic, 1 = creative)&#13;&#10;        top_p: nucleus sampling (controls diversity)&#13;&#10;&#13;&#10;    Returns:&#13;&#10;        Cached LLM instance for better performance&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    # Configuration for faster responses&#13;&#10;    llm = ChatGoogleGenerativeAI(&#13;&#10;        model=MODEL_NAME,&#13;&#10;        api_key=google_api_key,&#13;&#10;        temperature=temperature,&#13;&#10;        top_p=top_p,&#13;&#10;        max_tokens=1024,&#13;&#10;        timeout=15,&#13;&#10;&#13;&#10;    )&#13;&#10;&#13;&#10;    return llm&#13;&#10;&#13;&#10;&#13;&#10;@lru_cache(maxsize=4)&#13;&#10;def get_openai_model(&#13;&#10;    temperature: float = 0,&#13;&#10;    top_p: float = 1.0,&#13;&#10;):&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    Create a cached, configured ChatOpenAI model.&#13;&#10;&#13;&#10;    Params:&#13;&#10;        temperature: randomness (0 = deterministic, 1 = creative)&#13;&#10;        top_p: nucleus sampling (controls diversity)&#13;&#10;&#13;&#10;    Returns:&#13;&#10;        Cached OpenAI LLM instance for better performance&#13;&#10;    &quot;&quot;&quot;&#13;&#10;    if not openai_api_key:&#13;&#10;        logger.error(&quot;❌ OPENAI_API_KEY not found in environment variables&quot;)&#13;&#10;        raise ValueError(&quot;❌ OPENAI_API_KEY not found in environment variables.&quot;)&#13;&#10;&#13;&#10;    logger.info(f&quot; Initializing OpenAI model: {OPENAI_MODEL_NAME}&quot;)&#13;&#10;    logger.info(f&quot; Base URL: {openai_base_url}&quot;)&#13;&#10;    logger.info(f&quot; API Key present: {bool(openai_api_key)}&quot;)&#13;&#10;&#13;&#10;    try:&#13;&#10;        llm = ChatOpenAI(&#13;&#10;            model=OPENAI_MODEL_NAME,&#13;&#10;            api_key=openai_api_key,&#13;&#10;            base_url=openai_base_url,&#13;&#10;            temperature=temperature,&#13;&#10;            top_p=top_p,&#13;&#10;            max_tokens=2048,&#13;&#10;            timeout=60,  # Increased timeout for Cloud Run&#13;&#10;            seed=1234,&#13;&#10;            max_retries=3,  # Added retry&#13;&#10;            request_timeout=60,  # Added request timeout&#13;&#10;        )&#13;&#10;        logger.info(&quot;✅ OpenAI model initialized successfully&quot;)&#13;&#10;        return llm&#13;&#10;    except Exception as e:&#13;&#10;        logger.error(f&quot;❌ Failed to initialize OpenAI model: {str(e)}&quot;)&#13;&#10;        raise" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/apps/cv_analysis_agent/services/ai_checker_resume_service.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/apps/cv_analysis_agent/services/ai_checker_resume_service.py" />
              <option name="originalContent" value="import os&#10;import json&#10;import hashlib&#10;import time&#10;from agent_core.llm import get_openai_model&#10;from . import clean_json_output, extract_text&#10;&#10;&#10;&#10;#  Fixed explanations for each section (like Cake.me)&#10;FIELD_EXPLANATIONS = {&#10;    &quot;content&quot;: (&#10;        &quot;This section ensures your resume includes clearly quantifiable results &quot;&#10;        &quot;and is free of grammatical or spelling errors. These improvements make your resume &quot;&#10;        &quot;more impressive and memorable to recruiters.&quot;&#10;    ),&#10;    &quot;skills&quot;: (&#10;        &quot;This section compares your resume against the job description to identify missing or &quot;&#10;        &quot;underrepresented skills. Adding these skills increases the chance of passing ATS filters.&quot;&#10;    ),&#10;    &quot;format&quot;: (&#10;        &quot;This section checks date formats, resume length, and the effective use of bullet points &quot;&#10;        &quot;to ensure your resume is clean, concise, and ATS-friendly.&quot;&#10;    ),&#10;    &quot;sections&quot;: (&#10;        &quot;This section checks whether all required fields — such as contact information and &quot;&#10;        &quot;work experience — are properly filled and formatted, ensuring ATS compatibility.&quot;&#10;    ),&#10;    &quot;style&quot;: (&#10;        &quot;This section helps adjust the tone and wording of your resume to align with the job &quot;&#10;        &quot;description while avoiding generic or cliché phrases, resulting in a more professional and impactful resume.&quot;&#10;    ),&#10;}&#10;&#10;#  Footer section — personalized improvement recommendations&#10;RECOMMENDATION_HEADER = {&#10;    &quot;title&quot;: &quot;Recommendations for You&quot;,&#10;    &quot;description&quot;: (&#10;        &quot;Here are some personalized recommendations to help you improve your resume and &quot;&#10;        &quot;increase your ATS score, as well as suggestions to guide your skill or career development.&quot;&#10;    ),&#10;}&#10;&#10;CACHE_DIR = os.path.join(os.path.dirname(__file__), &quot;../../../.cache/ai_cv_analysis&quot;)&#10;CACHE_TTL_SECONDS = int(os.getenv(&quot;AI_CV_ANALYSIS_CACHE_TTL&quot;, &quot;604800&quot;))  # 7 days default&#10;CACHE_VERSION = os.getenv(&quot;AI_CV_ANALYSIS_CACHE_VERSION&quot;, &quot;v1&quot;)&#10;&#10;&#10;def _ensure_cache_dir():&#10;    os.makedirs(os.path.abspath(CACHE_DIR), exist_ok=True)&#10;&#10;&#10;def _cache_path(key: str) -&gt; str:&#10;    _ensure_cache_dir()&#10;    return os.path.abspath(os.path.join(CACHE_DIR, f&quot;{key}.json&quot;))&#10;&#10;&#10;def _cache_get(key: str):&#10;    try:&#10;        path = _cache_path(key)&#10;        if not os.path.exists(path):&#10;            return None&#10;        # TTL check&#10;        if CACHE_TTL_SECONDS &gt; 0:&#10;            mtime = os.path.getmtime(path)&#10;            if time.time() - mtime &gt; CACHE_TTL_SECONDS:&#10;                return None&#10;        with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;            data = json.load(f)&#10;        # mark cache hit&#10;        if isinstance(data, dict):&#10;            data.setdefault(&quot;cache&quot;, {})&#10;            data[&quot;cache&quot;].update({&quot;hit&quot;: True, &quot;key&quot;: key, &quot;age_seconds&quot;: int(time.time() - os.path.getmtime(path))})&#10;        return data&#10;    except Exception:&#10;        return None&#10;&#10;&#10;def _cache_set(key: str, result: dict):&#10;    path = _cache_path(key)&#10;    tmp = path + &quot;.tmp&quot;&#10;    # include cache metadata&#10;    to_write = dict(result)&#10;    to_write.setdefault(&quot;cache&quot;, {})&#10;    to_write[&quot;cache&quot;].update({&quot;hit&quot;: False, &quot;key&quot;: key, &quot;stored_at&quot;: int(time.time())})&#10;    with open(tmp, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        json.dump(to_write, f, ensure_ascii=False)&#10;    os.replace(tmp, path)&#10;&#10;&#10;def try_get_cached_result(cv_file, job_description: str):&#10;    &quot;&quot;&quot;&#10;    Try to get cached result without calling AI API.&#10;    Returns cached result if found, None otherwise.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Extract and normalize CV text&#10;        cv_text = extract_text.extract_text(cv_file)&#10;        cv_summary = cv_text[:2000]&#10;        jd_summary = job_description[:1500]&#10;&#10;        # Build same cache key as analyze_cv_vs_jd&#10;        model_config = {&#10;            &quot;model&quot;: os.getenv(&quot;OPENAI_MODEL&quot;, &quot;gpt-4o-mini&quot;),&#10;            &quot;temperature&quot;: 0.5,&#10;            &quot;top_p&quot;: 0.9,&#10;        }&#10;&#10;        prompt = f&quot;&quot;&quot;Analyze CV vs Job. Score 0-100. Return ONLY valid JSON.&#10;&#10;{{&#10;  &quot;summary&quot;: {{&quot;overall_match&quot;: &lt;int&gt;, &quot;overview_comment&quot;: &quot;&lt;text&gt;&quot;, &quot;strengths&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;], &quot;improvements&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;]}},&#10;  &quot;content&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;measurable_results&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;grammar_issues&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;skills&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;technical&quot;: {{&quot;matched&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;], &quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;]}}, &quot;soft&quot;: {{&quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;]}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;format&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;checks&quot;: {{&quot;date_format&quot;: &quot;PASS|FAIL&quot;, &quot;length&quot;: &quot;PASS|FAIL&quot;, &quot;bullet_points&quot;: &quot;PASS|FAIL&quot;}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;sections&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;missing&quot;: [&quot;section1&quot;, &quot;section2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;style&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;tone&quot;: [&quot;issue1&quot;, &quot;issue2&quot;], &quot;buzzwords&quot;: [&quot;word1&quot;, &quot;word2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;recommendations&quot;: {{&quot;items&quot;: [&quot;rec1&quot;, &quot;rec2&quot;, &quot;rec3&quot;]}},&#10;  &quot;overall_score&quot;: &lt;int&gt;,&#10;  &quot;overall_comment&quot;: &quot;&lt;text max 50 words&gt;&quot;&#10;}}&#10;&#10;RESUME:&#10;{cv_summary}&#10;&#10;JOB DESCRIPTION:&#10;{jd_summary}&#10;&#10;Analyze and return complete JSON:&quot;&quot;&quot;&#10;&#10;        key_basis = json.dumps({&#10;            &quot;version&quot;: CACHE_VERSION,&#10;            &quot;prompt&quot;: prompt,&#10;            &quot;config&quot;: model_config,&#10;        }, ensure_ascii=False)&#10;        key = hashlib.sha256(key_basis.encode(&quot;utf-8&quot;)).hexdigest()&#10;&#10;        # Try to get from cache&#10;        return _cache_get(key)&#10;    except Exception as e:&#10;        print(f&quot;⚠️ Cache check error: {e}&quot;)&#10;        return None&#10;&#10;&#10;def analyze_cv_vs_jd(cv_file, job_description: str, force_refresh: bool = False):&#10;    &quot;&quot;&quot;&#10;    AI-powered CV + JD analysis (Cake.me style)&#10;    Returns structured JSON with sections:&#10;    summary, content, skills, format, sections, style, recommendations, overall_score&#10;    &quot;&quot;&quot;&#10;&#10;    # 1️⃣ Extract and normalize CV text&#10;    cv_text = extract_text.extract_text(cv_file)&#10;&#10;    # 2️⃣ Tóm tắt CV và JD để giảm token&#10;    cv_summary = cv_text[:2000]&#10;    jd_summary = job_description[:1500]&#10;&#10;    # 3️⃣ Build the analysis prompt - Tối ưu cho OpenAI&#10;    prompt = f&quot;&quot;&quot;Analyze CV vs Job. Score 0-100. Return ONLY valid JSON.&#10;&#10;{{&#10;  &quot;summary&quot;: {{&quot;overall_match&quot;: &lt;int&gt;, &quot;overview_comment&quot;: &quot;&lt;text&gt;&quot;, &quot;strengths&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;], &quot;improvements&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;]}},&#10;  &quot;content&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;measurable_results&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;grammar_issues&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;skills&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;technical&quot;: {{&quot;matched&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;], &quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;]}}, &quot;soft&quot;: {{&quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;]}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;format&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;checks&quot;: {{&quot;date_format&quot;: &quot;PASS|FAIL&quot;, &quot;length&quot;: &quot;PASS|FAIL&quot;, &quot;bullet_points&quot;: &quot;PASS|FAIL&quot;}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;sections&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;missing&quot;: [&quot;section1&quot;, &quot;section2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;style&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;tone&quot;: [&quot;issue1&quot;, &quot;issue2&quot;], &quot;buzzwords&quot;: [&quot;word1&quot;, &quot;word2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;recommendations&quot;: {{&quot;items&quot;: [&quot;rec1&quot;, &quot;rec2&quot;, &quot;rec3&quot;]}},&#10;  &quot;overall_score&quot;: &lt;int&gt;,&#10;  &quot;overall_comment&quot;: &quot;&lt;text max 50 words&gt;&quot;&#10;}}&#10;&#10;RESUME:&#10;{cv_summary}&#10;&#10;JOB DESCRIPTION:&#10;{jd_summary}&#10;&#10;Analyze and return complete JSON:&quot;&quot;&quot;&#10;&#10;    # 3.5️⃣ Cache key by full prompt hash + config (prompt caching)&#10;    # Include cache version, model/config to avoid stale collisions after changes&#10;    model_config = {&#10;        &quot;model&quot;: os.getenv(&quot;OPENAI_MODEL&quot;, &quot;gpt-4o-mini&quot;),&#10;        &quot;temperature&quot;: 0.5,&#10;        &quot;top_p&quot;: 0.9,&#10;    }&#10;    key_basis = json.dumps({&#10;        &quot;version&quot;: CACHE_VERSION,&#10;        &quot;prompt&quot;: prompt,&#10;        &quot;config&quot;: model_config,&#10;    }, ensure_ascii=False)&#10;    key = hashlib.sha256(key_basis.encode(&quot;utf-8&quot;)).hexdigest()&#10;&#10;    if not force_refresh:&#10;        cached = _cache_get(key)&#10;        if cached:&#10;            print(f&quot;️ Cache HIT: {key} (age={cached.get('cache',{}).get('age_seconds','?')}s)&quot;)&#10;            return cached&#10;    else:&#10;        print(&quot;️ Cache BYPASSED (force_refresh=True)&quot;)&#10;&#10;    # 4️⃣ Run OpenAI API&#10;    model = get_openai_model(temperature=model_config[&quot;temperature&quot;], top_p=model_config[&quot;top_p&quot;])&#10;&#10;    try:&#10;        # Gọi OpenAI invoke() thay vì generate_content()&#10;        response = model.invoke(prompt)&#10;        raw_text = response.content if hasattr(response, 'content') else str(response)&#10;        raw_text = raw_text.strip()&#10;&#10;        # OpenAI không có usage_metadata như Gemini&#10;        # Tính token ước tính (OpenAI ~4 chars = 1 token)&#10;        input_tokens = len(prompt) // 4&#10;        output_tokens = len(raw_text) // 4&#10;        total_tokens = input_tokens + output_tokens&#10;&#10;        print(f&quot; Input tokens (estimated): {input_tokens}&quot;)&#10;        print(f&quot; Output tokens (estimated): {output_tokens}&quot;)&#10;        print(f&quot; Total tokens (estimated): {total_tokens}&quot;)&#10;&#10;    except Exception as e:&#10;        print(f&quot;❌ OpenAI Error: {str(e)}&quot;)&#10;&#10;        # Fallback JSON nếu lỗi&#10;        raw_text = json.dumps({&#10;            &quot;summary&quot;: {&#10;                &quot;overall_match&quot;: 50,&#10;                &quot;overview_comment&quot;: &quot;Analysis incomplete&quot;,&#10;                &quot;strengths&quot;: [&quot;Resume submitted&quot;],&#10;                &quot;improvements&quot;: [&quot;Please try again&quot;]&#10;            },&#10;            &quot;content&quot;: {&quot;score&quot;: 50, &quot;measurable_results&quot;: [], &quot;grammar_issues&quot;: [], &quot;tips&quot;: []},&#10;            &quot;skills&quot;: {&quot;score&quot;: 50, &quot;technical&quot;: {&quot;matched&quot;: [], &quot;missing&quot;: []}, &quot;soft&quot;: {&quot;missing&quot;: []}, &quot;tips&quot;: []},&#10;            &quot;format&quot;: {&quot;score&quot;: 50, &quot;checks&quot;: {&quot;date_format&quot;: &quot;PASS&quot;, &quot;length&quot;: &quot;PASS&quot;, &quot;bullet_points&quot;: &quot;PASS&quot;}, &quot;tips&quot;: []},&#10;            &quot;sections&quot;: {&quot;score&quot;: 50, &quot;missing&quot;: [], &quot;tips&quot;: []},&#10;            &quot;style&quot;: {&quot;score&quot;: 50, &quot;tone&quot;: [], &quot;buzzwords&quot;: [], &quot;tips&quot;: []},&#10;            &quot;recommendations&quot;: {&quot;items&quot;: [&quot;Please retry&quot;]},&#10;            &quot;overall_score&quot;: 50,&#10;            &quot;overall_comment&quot;: &quot;Error occurred&quot;&#10;        })&#10;&#10;        input_tokens = len(prompt) // 4&#10;        output_tokens = len(raw_text) // 4&#10;        total_tokens = input_tokens + output_tokens&#10;&#10;    # 5️⃣ Clean and parse the JSON safely&#10;    result = clean_json_output.clean_json_output(raw_text)&#10;&#10;    # 6️⃣ Inject static explanations into each ATS field&#10;    for key_field, desc in FIELD_EXPLANATIONS.items():&#10;        if key_field in result:&#10;            result[key_field][&quot;description&quot;] = desc&#10;        else:&#10;            result[key_field] = {&quot;description&quot;: desc}&#10;&#10;    # 7️⃣ Ensure recommendations section exists&#10;    if &quot;recommendations&quot; not in result:&#10;        result[&quot;recommendations&quot;] = {&quot;items&quot;: []}&#10;    result[&quot;recommendations&quot;].update({&#10;        &quot;title&quot;: &quot;Recommendations for You&quot;,&#10;        &quot;description&quot;: (&#10;            &quot;Here are some personalized recommendations to help you improve your resume and &quot;&#10;            &quot;increase your ATS score, as well as suggestions to guide your skill or career development.&quot;&#10;        ),&#10;    })&#10;&#10;    # 8️⃣ Add token usage info to result&#10;    result[&quot;token_usage&quot;] = {&#10;        &quot;input_tokens&quot;: input_tokens,&#10;        &quot;output_tokens&quot;: output_tokens,&#10;        &quot;total_tokens&quot;: total_tokens,&#10;        &quot;estimated&quot;: True,&#10;        &quot;cache&quot;: {&#10;            &quot;key&quot;: key,&#10;            &quot;hit&quot;: False if force_refresh else False,&#10;        },&#10;    }&#10;&#10;    # 9️⃣ Save to cache&#10;    _cache_set(key, result)&#10;&#10;    return result&#10;" />
              <option name="updatedContent" value="import os&#10;import json&#10;import hashlib&#10;import time&#10;import logging&#10;from agent_core.llm import get_openai_model&#10;from . import clean_json_output, extract_text&#10;&#10;logger = logging.getLogger(__name__)&#10;&#10;#  Fixed explanations for each section (like Cake.me)&#10;FIELD_EXPLANATIONS = {&#10;    &quot;content&quot;: (&#10;        &quot;This section ensures your resume includes clearly quantifiable results &quot;&#10;        &quot;and is free of grammatical or spelling errors. These improvements make your resume &quot;&#10;        &quot;more impressive and memorable to recruiters.&quot;&#10;    ),&#10;    &quot;skills&quot;: (&#10;        &quot;This section compares your resume against the job description to identify missing or &quot;&#10;        &quot;underrepresented skills. Adding these skills increases the chance of passing ATS filters.&quot;&#10;    ),&#10;    &quot;format&quot;: (&#10;        &quot;This section checks date formats, resume length, and the effective use of bullet points &quot;&#10;        &quot;to ensure your resume is clean, concise, and ATS-friendly.&quot;&#10;    ),&#10;    &quot;sections&quot;: (&#10;        &quot;This section checks whether all required fields — such as contact information and &quot;&#10;        &quot;work experience — are properly filled and formatted, ensuring ATS compatibility.&quot;&#10;    ),&#10;    &quot;style&quot;: (&#10;        &quot;This section helps adjust the tone and wording of your resume to align with the job &quot;&#10;        &quot;description while avoiding generic or cliché phrases, resulting in a more professional and impactful resume.&quot;&#10;    ),&#10;}&#10;&#10;#  Footer section — personalized improvement recommendations&#10;RECOMMENDATION_HEADER = {&#10;    &quot;title&quot;: &quot;Recommendations for You&quot;,&#10;    &quot;description&quot;: (&#10;        &quot;Here are some personalized recommendations to help you improve your resume and &quot;&#10;        &quot;increase your ATS score, as well as suggestions to guide your skill or career development.&quot;&#10;    ),&#10;}&#10;&#10;CACHE_DIR = os.path.join(os.path.dirname(__file__), &quot;../../../.cache/ai_cv_analysis&quot;)&#10;CACHE_TTL_SECONDS = int(os.getenv(&quot;AI_CV_ANALYSIS_CACHE_TTL&quot;, &quot;604800&quot;))  # 7 days default&#10;CACHE_VERSION = os.getenv(&quot;AI_CV_ANALYSIS_CACHE_VERSION&quot;, &quot;v1&quot;)&#10;&#10;&#10;def _ensure_cache_dir():&#10;    os.makedirs(os.path.abspath(CACHE_DIR), exist_ok=True)&#10;&#10;&#10;def _cache_path(key: str) -&gt; str:&#10;    _ensure_cache_dir()&#10;    return os.path.abspath(os.path.join(CACHE_DIR, f&quot;{key}.json&quot;))&#10;&#10;&#10;def _cache_get(key: str):&#10;    try:&#10;        path = _cache_path(key)&#10;        if not os.path.exists(path):&#10;            return None&#10;        # TTL check&#10;        if CACHE_TTL_SECONDS &gt; 0:&#10;            mtime = os.path.getmtime(path)&#10;            if time.time() - mtime &gt; CACHE_TTL_SECONDS:&#10;                return None&#10;        with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;            data = json.load(f)&#10;        # mark cache hit&#10;        if isinstance(data, dict):&#10;            data.setdefault(&quot;cache&quot;, {})&#10;            data[&quot;cache&quot;].update({&quot;hit&quot;: True, &quot;key&quot;: key, &quot;age_seconds&quot;: int(time.time() - os.path.getmtime(path))})&#10;        return data&#10;    except Exception:&#10;        return None&#10;&#10;&#10;def _cache_set(key: str, result: dict):&#10;    path = _cache_path(key)&#10;    tmp = path + &quot;.tmp&quot;&#10;    # include cache metadata&#10;    to_write = dict(result)&#10;    to_write.setdefault(&quot;cache&quot;, {})&#10;    to_write[&quot;cache&quot;].update({&quot;hit&quot;: False, &quot;key&quot;: key, &quot;stored_at&quot;: int(time.time())})&#10;    with open(tmp, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        json.dump(to_write, f, ensure_ascii=False)&#10;    os.replace(tmp, path)&#10;&#10;&#10;def try_get_cached_result(cv_file, job_description: str):&#10;    &quot;&quot;&quot;&#10;    Try to get cached result without calling AI API.&#10;    Returns cached result if found, None otherwise.&#10;    &quot;&quot;&quot;&#10;    try:&#10;        # Extract and normalize CV text&#10;        cv_text = extract_text.extract_text(cv_file)&#10;        cv_summary = cv_text[:2000]&#10;        jd_summary = job_description[:1500]&#10;&#10;        # Build same cache key as analyze_cv_vs_jd&#10;        model_config = {&#10;            &quot;model&quot;: os.getenv(&quot;OPENAI_MODEL&quot;, &quot;gpt-4o-mini&quot;),&#10;            &quot;temperature&quot;: 0.5,&#10;            &quot;top_p&quot;: 0.9,&#10;        }&#10;&#10;        prompt = f&quot;&quot;&quot;Analyze CV vs Job. Score 0-100. Return ONLY valid JSON.&#10;&#10;{{&#10;  &quot;summary&quot;: {{&quot;overall_match&quot;: &lt;int&gt;, &quot;overview_comment&quot;: &quot;&lt;text&gt;&quot;, &quot;strengths&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;], &quot;improvements&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;]}},&#10;  &quot;content&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;measurable_results&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;grammar_issues&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;skills&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;technical&quot;: {{&quot;matched&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;], &quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;]}}, &quot;soft&quot;: {{&quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;]}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;format&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;checks&quot;: {{&quot;date_format&quot;: &quot;PASS|FAIL&quot;, &quot;length&quot;: &quot;PASS|FAIL&quot;, &quot;bullet_points&quot;: &quot;PASS|FAIL&quot;}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;sections&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;missing&quot;: [&quot;section1&quot;, &quot;section2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;style&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;tone&quot;: [&quot;issue1&quot;, &quot;issue2&quot;], &quot;buzzwords&quot;: [&quot;word1&quot;, &quot;word2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;recommendations&quot;: {{&quot;items&quot;: [&quot;rec1&quot;, &quot;rec2&quot;, &quot;rec3&quot;]}},&#10;  &quot;overall_score&quot;: &lt;int&gt;,&#10;  &quot;overall_comment&quot;: &quot;&lt;text max 50 words&gt;&quot;&#10;}}&#10;&#10;RESUME:&#10;{cv_summary}&#10;&#10;JOB DESCRIPTION:&#10;{jd_summary}&#10;&#10;Analyze and return complete JSON:&quot;&quot;&quot;&#10;&#10;        key_basis = json.dumps({&#10;            &quot;version&quot;: CACHE_VERSION,&#10;            &quot;prompt&quot;: prompt,&#10;            &quot;config&quot;: model_config,&#10;        }, ensure_ascii=False)&#10;        key = hashlib.sha256(key_basis.encode(&quot;utf-8&quot;)).hexdigest()&#10;&#10;        # Try to get from cache&#10;        return _cache_get(key)&#10;    except Exception as e:&#10;        print(f&quot;⚠️ Cache check error: {e}&quot;)&#10;        return None&#10;&#10;&#10;def analyze_cv_vs_jd(cv_file, job_description: str, force_refresh: bool = False):&#10;    &quot;&quot;&quot;&#10;    AI-powered CV + JD analysis (Cake.me style)&#10;    Returns structured JSON with sections:&#10;    summary, content, skills, format, sections, style, recommendations, overall_score&#10;    &quot;&quot;&quot;&#10;&#10;    # 1️⃣ Extract and normalize CV text&#10;    cv_text = extract_text.extract_text(cv_file)&#10;&#10;    # 2️⃣ Tóm tắt CV và JD để giảm token&#10;    cv_summary = cv_text[:2000]&#10;    jd_summary = job_description[:1500]&#10;&#10;    # 3️⃣ Build the analysis prompt - Tối ưu cho OpenAI&#10;    prompt = f&quot;&quot;&quot;Analyze CV vs Job. Score 0-100. Return ONLY valid JSON.&#10;&#10;{{&#10;  &quot;summary&quot;: {{&quot;overall_match&quot;: &lt;int&gt;, &quot;overview_comment&quot;: &quot;&lt;text&gt;&quot;, &quot;strengths&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;], &quot;improvements&quot;: [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;]}},&#10;  &quot;content&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;measurable_results&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;grammar_issues&quot;: [&quot;item1&quot;, &quot;item2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;skills&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;technical&quot;: {{&quot;matched&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;], &quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;, &quot;skill3&quot;]}}, &quot;soft&quot;: {{&quot;missing&quot;: [&quot;skill1&quot;, &quot;skill2&quot;]}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;format&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;checks&quot;: {{&quot;date_format&quot;: &quot;PASS|FAIL&quot;, &quot;length&quot;: &quot;PASS|FAIL&quot;, &quot;bullet_points&quot;: &quot;PASS|FAIL&quot;}}, &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;sections&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;missing&quot;: [&quot;section1&quot;, &quot;section2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;style&quot;: {{&quot;score&quot;: &lt;int&gt;, &quot;tone&quot;: [&quot;issue1&quot;, &quot;issue2&quot;], &quot;buzzwords&quot;: [&quot;word1&quot;, &quot;word2&quot;], &quot;tips&quot;: [&quot;item1&quot;, &quot;item2&quot;]}},&#10;  &quot;recommendations&quot;: {{&quot;items&quot;: [&quot;rec1&quot;, &quot;rec2&quot;, &quot;rec3&quot;]}},&#10;  &quot;overall_score&quot;: &lt;int&gt;,&#10;  &quot;overall_comment&quot;: &quot;&lt;text max 50 words&gt;&quot;&#10;}}&#10;&#10;RESUME:&#10;{cv_summary}&#10;&#10;JOB DESCRIPTION:&#10;{jd_summary}&#10;&#10;Analyze and return complete JSON:&quot;&quot;&quot;&#10;&#10;    # 3.5️⃣ Cache key by full prompt hash + config (prompt caching)&#10;    # Include cache version, model/config to avoid stale collisions after changes&#10;    model_config = {&#10;        &quot;model&quot;: os.getenv(&quot;OPENAI_MODEL&quot;, &quot;gpt-4o-mini&quot;),&#10;        &quot;temperature&quot;: 0.5,&#10;        &quot;top_p&quot;: 0.9,&#10;    }&#10;    key_basis = json.dumps({&#10;        &quot;version&quot;: CACHE_VERSION,&#10;        &quot;prompt&quot;: prompt,&#10;        &quot;config&quot;: model_config,&#10;    }, ensure_ascii=False)&#10;    key = hashlib.sha256(key_basis.encode(&quot;utf-8&quot;)).hexdigest()&#10;&#10;    if not force_refresh:&#10;        cached = _cache_get(key)&#10;        if cached:&#10;            logger.info(f&quot;️ Cache HIT: {key} (age={cached.get('cache',{}).get('age_seconds','?')}s)&quot;)&#10;            print(f&quot;️ Cache HIT: {key} (age={cached.get('cache',{}).get('age_seconds','?')}s)&quot;)&#10;            return cached&#10;    else:&#10;        logger.info(&quot;️ Cache BYPASSED (force_refresh=True)&quot;)&#10;        print(&quot;️ Cache BYPASSED (force_refresh=True)&quot;)&#10;&#10;    # 4️⃣ Run OpenAI API&#10;    logger.info(&quot; Calling OpenAI API...&quot;)&#10;    logger.info(f&quot; Prompt length: {len(prompt)} chars&quot;)&#10;    &#10;    try:&#10;        model = get_openai_model(temperature=model_config[&quot;temperature&quot;], top_p=model_config[&quot;top_p&quot;])&#10;        logger.info(&quot;✅ OpenAI model obtained successfully&quot;)&#10;    except Exception as e:&#10;        logger.error(f&quot;❌ Failed to get OpenAI model: {str(e)}&quot;, exc_info=True)&#10;        raise&#10;&#10;    try:&#10;        # Gọi OpenAI invoke() thay vì generate_content()&#10;        logger.info(&quot; Invoking OpenAI API...&quot;)&#10;        response = model.invoke(prompt)&#10;        raw_text = response.content if hasattr(response, 'content') else str(response)&#10;        raw_text = raw_text.strip()&#10;&#10;        # OpenAI không có usage_metadata như Gemini&#10;        # Tính token ước tính (OpenAI ~4 chars = 1 token)&#10;        input_tokens = len(prompt) // 4&#10;        output_tokens = len(raw_text) // 4&#10;        total_tokens = input_tokens + output_tokens&#10;&#10;        print(f&quot; Input tokens (estimated): {input_tokens}&quot;)&#10;        print(f&quot; Output tokens (estimated): {output_tokens}&quot;)&#10;        print(f&quot; Total tokens (estimated): {total_tokens}&quot;)&#10;&#10;    except Exception as e:&#10;        error_type = type(e).__name__&#10;        error_msg = str(e)&#10;        logger.error(f&quot;❌ OpenAI Error ({error_type}): {error_msg}&quot;, exc_info=True)&#10;        print(f&quot;❌ OpenAI Error ({error_type}): {error_msg}&quot;)&#10;        &#10;        # Log thêm thông tin môi trường&#10;        logger.error(f&quot; OPENAI_API_KEY present: {bool(os.getenv('OPENAI_API_KEY'))}&quot;)&#10;        logger.error(f&quot; OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL', 'Not set')}&quot;)&#10;        print(f&quot; OPENAI_API_KEY present: {bool(os.getenv('OPENAI_API_KEY'))}&quot;)&#10;        print(f&quot; OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL', 'Not set')}&quot;)&#10;&#10;        # Fallback JSON nếu lỗi&#10;        raw_text = json.dumps({&#10;            &quot;summary&quot;: {&#10;                &quot;overall_match&quot;: 50,&#10;                &quot;overview_comment&quot;: f&quot;Analysis incomplete due to API error: {error_type}&quot;,&#10;                &quot;strengths&quot;: [&quot;Resume submitted&quot;],&#10;                &quot;improvements&quot;: [&quot;API connection failed - please try again&quot;]&#10;            },&#10;            &quot;content&quot;: {&quot;score&quot;: 50, &quot;measurable_results&quot;: [], &quot;grammar_issues&quot;: [], &quot;tips&quot;: [&quot;API error occurred&quot;]},&#10;            &quot;skills&quot;: {&quot;score&quot;: 50, &quot;technical&quot;: {&quot;matched&quot;: [], &quot;missing&quot;: []}, &quot;soft&quot;: {&quot;missing&quot;: []}, &quot;tips&quot;: [&quot;API error occurred&quot;]},&#10;            &quot;format&quot;: {&quot;score&quot;: 50, &quot;checks&quot;: {&quot;date_format&quot;: &quot;PASS&quot;, &quot;length&quot;: &quot;PASS&quot;, &quot;bullet_points&quot;: &quot;PASS&quot;}, &quot;tips&quot;: []},&#10;            &quot;sections&quot;: {&quot;score&quot;: 50, &quot;missing&quot;: [], &quot;tips&quot;: []},&#10;            &quot;style&quot;: {&quot;score&quot;: 50, &quot;tone&quot;: [], &quot;buzzwords&quot;: [], &quot;tips&quot;: []},&#10;            &quot;recommendations&quot;: {&quot;items&quot;: [f&quot;API Error: {error_msg[:100]}&quot;]},&#10;            &quot;overall_score&quot;: 50,&#10;            &quot;overall_comment&quot;: f&quot;Error: {error_type}&quot;,&#10;            &quot;error_details&quot;: {&#10;                &quot;error_type&quot;: error_type,&#10;                &quot;error_message&quot;: error_msg,&#10;                &quot;timestamp&quot;: int(time.time())&#10;            }&#10;        })&#10;&#10;        input_tokens = len(prompt) // 4&#10;        output_tokens = len(raw_text) // 4&#10;        total_tokens = input_tokens + output_tokens&#10;&#10;    # 5️⃣ Clean and parse the JSON safely&#10;    result = clean_json_output.clean_json_output(raw_text)&#10;&#10;    # 6️⃣ Inject static explanations into each ATS field&#10;    for key_field, desc in FIELD_EXPLANATIONS.items():&#10;        if key_field in result:&#10;            result[key_field][&quot;description&quot;] = desc&#10;        else:&#10;            result[key_field] = {&quot;description&quot;: desc}&#10;&#10;    # 7️⃣ Ensure recommendations section exists&#10;    if &quot;recommendations&quot; not in result:&#10;        result[&quot;recommendations&quot;] = {&quot;items&quot;: []}&#10;    result[&quot;recommendations&quot;].update({&#10;        &quot;title&quot;: &quot;Recommendations for You&quot;,&#10;        &quot;description&quot;: (&#10;            &quot;Here are some personalized recommendations to help you improve your resume and &quot;&#10;            &quot;increase your ATS score, as well as suggestions to guide your skill or career development.&quot;&#10;        ),&#10;    })&#10;&#10;    # 8️⃣ Add token usage info to result&#10;    result[&quot;token_usage&quot;] = {&#10;        &quot;input_tokens&quot;: input_tokens,&#10;        &quot;output_tokens&quot;: output_tokens,&#10;        &quot;total_tokens&quot;: total_tokens,&#10;        &quot;estimated&quot;: True,&#10;        &quot;cache&quot;: {&#10;            &quot;key&quot;: key,&#10;            &quot;hit&quot;: False if force_refresh else False,&#10;        },&#10;    }&#10;&#10;    # 9️⃣ Save to cache&#10;    _cache_set(key, result)&#10;&#10;    return result" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/apps/recommendation_agent/services/job_service.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/apps/recommendation_agent/services/job_service.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Job Indexing Service for syncing jobs to Weaviate&#10;&quot;&quot;&quot;&#10;import logging&#10;&#10;logger = logging.getLogger(__name__)&#10;&#10;&#10;class JobIndexingService:&#10;    &quot;&quot;&quot;Service for indexing jobs to Weaviate&quot;&quot;&quot;&#10;    &#10;    @staticmethod&#10;    def sync_single_job_to_weaviate(job_id):&#10;        &quot;&quot;&quot;&#10;        Sync a single job to Weaviate&#10;        &#10;        Args:&#10;            job_id: ID of the job to sync&#10;            &#10;        Returns:&#10;            bool: True if successful, False otherwise&#10;        &quot;&quot;&quot;&#10;        try:&#10;            from ..models import JobPostings, JobDescription&#10;            from .weaviate_service import index_job, delete_job&#10;            &#10;            # Get job from database&#10;            try:&#10;                job = JobPostings.objects.select_related('recruiter').prefetch_related(&#10;                    'jobdescription_set__jd_skill'&#10;                ).get(id=job_id)&#10;            except JobPostings.DoesNotExist:&#10;                logger.warning(f&quot;Job {job_id} not found in database&quot;)&#10;                return False&#10;            &#10;            # If job is not active, remove from Weaviate&#10;            if job.status != 'ACTIVE':&#10;                return delete_job(job_id)&#10;            &#10;            # Prepare job data for indexing&#10;            skills = [jd.jd_skill.name for jd in job.jobdescription_set.all()]&#10;            &#10;            job_data = {&#10;                'job_id': job.id,&#10;                'title': job.title,&#10;                'description': job.description or '',&#10;                'skills': skills,&#10;                'company_name': job.recruiter.company_name,&#10;                'address': job.address or '',&#10;            }&#10;            &#10;            # Index to Weaviate&#10;            return index_job(job_data)&#10;            &#10;        except Exception as e:&#10;            logger.error(f&quot;Error syncing job {job_id} to Weaviate: {str(e)}&quot;)&#10;            return False&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>